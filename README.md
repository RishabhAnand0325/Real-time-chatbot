# Real-time AI Chatbot

A high-performance, asynchronous AI backend using Python (FastAPI), WebSockets, and Supabase, coupled with a visually stunning, glassmorphic Next.js frontend.

## ğŸš€ Features

* **Real-time Streaming**: Instant LLM token delivery via WebSockets for low-latency interactions.
* **Complex LLM Interaction**: Implements function calling (simulated weather tool) to demonstrate capabilities beyond simple Q&A.
* **Async Persistence**: Non-blocking data storage to Supabase (PostgreSQL) ensuring smooth streaming.
* **Post-Session Automation**: Automatically triggers an async job to analyze and summarize the conversation upon disconnection.
* **Glassmorphism UI**: A modern, responsive interface built with Next.js 14, Tailwind CSS, and Framer Motion.

## ğŸ› ï¸ Tech Stack

* **Backend**: Python 3.10+, FastAPI, Uvicorn, AsyncOpenAI, Supabase-py
* **Frontend**: Next.js 14, TypeScript, Tailwind CSS, Framer Motion
* **Database**: PostgreSQL (Supabase)

---

## ğŸ—„ï¸ Database Schema Setup

Before running the application, execute the following SQL commands in your **Supabase SQL Editor** to create the required schema:

```sql
-- Table 1: Session Metadata
-- Stores high-level details for each chat session
CREATE TABLE session_metadata (
    session_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id TEXT NOT NULL,
    start_time TIMESTAMPTZ DEFAULT NOW(),
    end_time TIMESTAMPTZ,
    session_summary TEXT,
    duration_seconds INTEGER
);

-- Table 2: Detailed Event Log
-- Stores a granular, chronological log of all events (messages, tool calls, etc.)
CREATE TABLE event_log (
    event_id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
    session_id UUID REFERENCES session_metadata(session_id),
    timestamp TIMESTAMPTZ DEFAULT NOW(),
    event_type TEXT NOT NULL, -- e.g., 'user_message', 'ai_response', 'tool_call'
    content TEXT
);
```

# Real-time AI Backend (Tecnvirons Assignment)

## âš™ï¸ Installation & Setup

This project is divided into two parts: the Python backend and the Next.js frontend. You will need two terminal windows to run them simultaneously.

### 1. Backend Setup (FastAPI)

1.  **Navigate to the backend directory:**
    ```bash
    cd backend
    ```

2.  **Create and activate a virtual environment (Recommended):**
    ```bash
    # Windows
    python -m venv venv
    venv\Scripts\activate

    # Mac/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Install Python dependencies:**
    This installs FastAPI, Uvicorn, Supabase, and OpenAI libraries.
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure Environment Variables:**
    Create a file named `.env` in the `backend/` folder and add your API keys:
    ```env
    SUPABASE_URL="your_supabase_project_url"
    SUPABASE_KEY="your_supabase_service_role_key"
    OPENAI_API_KEY="sk-..."
    ```

5.  **Run the Server:**
    Start the asynchronous WebSocket server.
    ```bash
    uvicorn main:app --reload --port 8000
    ```
    *The backend will start at `http://localhost:8000`.*

### 2. Frontend Setup (Next.js)

1.  **Navigate to the frontend directory:**
    ```bash
    cd ../frontend
    ```

2.  **Install Node dependencies:**
    ```bash
    npm install
    ```

3.  **Configure Environment Variables:**
    Create a file named `.env.local` in the `frontend/` folder:
    ```env
    NEXT_PUBLIC_WEBSOCKET_URL=ws://localhost:8000/ws/session
    ```

4.  **Run the Development Server:**
    ```bash
    npm run dev
    ```

5.  **Access the Application:**
    Open your browser and navigate to **[http://localhost:3000](http://localhost:3000)**.

---

## ğŸ§  Design Rationale

This architecture was chosen to meet the requirements for high performance and real-time interaction.

### 1. WebSockets vs. HTTP
* **Decision:** We utilized WebSockets (`/ws/session`) instead of standard HTTP REST endpoints.
* **Reasoning:** Conversational AI requires a "typing" effect where tokens are streamed to the user instantly. HTTP polling is inefficient and adds latency. WebSockets provide a persistent, bi-directional connection that allows the backend to push tokens (`stream=True`) immediately as they are generated by the LLM.

### 2. Asynchronous Python (FastAPI + Asyncio)
* **Decision:** The backend is built on FastAPI using `async`/`await` syntax.
* **Reasoning:** The system performs heavy I/O operations (waiting for OpenAI responses and writing to the Supabase database). A synchronous framework would block the server thread during these waits, making it unresponsive to other users. Asyncio allows the server to handle multiple concurrent WebSocket connections efficiently while waiting for external APIs.

### 3. Dual-Table Database Schema
* **Decision:** Data is normalized into two tables: `session_metadata` and `event_log`.
* **Reasoning:**
    * **Performance:** Separating high-level metadata (start time, summary) from the granular logs prevents querying heavy text data when simply listing session history.
    * **Granularity:** The `event_log` captures every specific action (user message, AI token stream, tool call) in chronological order, providing a perfect audit trail for debugging and analysis.

### 4. Post-Session Automation
* **Decision:** Session summarization is triggered strictly *after* the WebSocket disconnects.
* **Reasoning:** Generating a summary using an LLM is a slow operation. By decoupling this from the live session and running it as a background task, we ensure the user's chatting experience remains perfectly smooth and is never blocked by "cleanup" tasks.

---

## ğŸ“‚ Project Structure

```text
project-root/
â”œâ”€â”€ backend/                  # Python FastAPI Backend
â”‚   â”œâ”€â”€ main.py               # Entry point, WebSocket handler, and orchestration
â”‚   â”œâ”€â”€ database.py           # Supabase async client and database queries
â”‚   â”œâ”€â”€ llm_service.py        # Logic for OpenAI streaming and tool calling
â”‚   â”œâ”€â”€ requirements.txt      # List of python dependencies
â”‚   â””â”€â”€ .env                  # Backend secrets (API keys)
â”‚
â””â”€â”€ frontend/                 # Next.js Frontend
    â”œâ”€â”€ app/
    â”‚   â”œâ”€â”€ page.tsx          # Main layout and UI container
    â”‚   â””â”€â”€ globals.css       # Global styles and Tailwind imports
    â”œâ”€â”€ components/
    â”‚   â”œâ”€â”€ ChatInterface.tsx # Core chat logic and WebSocket integration
    â”‚   â”œâ”€â”€ MessageBubble.tsx # UI component for individual messages
    â”‚   â””â”€â”€ TypingIndicator.tsx # Visual animation for AI loading state
    â”œâ”€â”€ tailwind.config.ts    # Tailwind CSS configuration
    â””â”€â”€ .env.local            # Frontend public variables
